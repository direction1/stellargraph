{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Deep Graph Infomax on CORA\n",
    "\n",
    "This demo demonstrates how to perform unsupervised training of a GCN model using the Deep Graph Infomax algorithm (https://arxiv.org/pdf/1809.10341.pdf) on the CORA dataset. \n",
    "\n",
    "As with all StellarGraph workflows: first we load the dataset, next we create our data generators, and then we train our model. We then take the embeddings created through unsupervised training and predict the node classes using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import CorruptedFullBatchNodeGenerator\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.layer import GCN, fullbatch_infomax_embedding_model, fullbatch_infomax_node_model\n",
    "from stellargraph import datasets\n",
    "from stellargraph.utils import plot_history\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "G, node_subjects = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Generators\n",
    " \n",
    " Now we create the data generators using `CorruptedFullBatchNodeGenerator`. `CorruptedFullBatchNodeGenerator` returns shuffled node features along with the regular node features and we train our model to discriminate between the two. \n",
    " \n",
    " Note that:\n",
    " \n",
    " - We typically pass all nodes to `generator.flow` because this is an unsupervised task\n",
    " - We don't pass `targets` to `generator.flow` because these are binary labels (true nodes, false nodes) that are created by `CorruptedFullBatchNodeGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n"
     ]
    }
   ],
   "source": [
    "generator = CorruptedFullBatchNodeGenerator(G,)\n",
    "gen = generator.flow(G.nodes(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Training\n",
    "\n",
    "We create an train our `GCNInfoMax` model. Note that the loss used here must always be `tf.nn.sigmoid_cross_entropy_with_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranricardo/PycharmProjects/current-stellargraph/stellargraph/stellargraph/core/experimental.py:88: ExperimentalWarning: fullbatch_infomax_node_model is experimental: lack of unit tests (see: https://github.com/stellargraph/stellargraph/issues/1003). It may be difficult to use and may have major changes at any time.\n",
      "  warnings.warn(direct_msg, ExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "gcn = GCN([512], activations=[tf.keras.layers.PReLU()], generator=generator)\n",
    "\n",
    "x_in, x_out = fullbatch_infomax_node_model(gcn)\n",
    "\n",
    "model = Model(inputs=x_in, outputs=x_out)\n",
    "model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor=\"loss\", min_delta=0, patience=20)\n",
    "history = model.fit(gen, epochs=1000, verbose=0, callbacks=[es])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings and Logistic Regression\n",
    "\n",
    "We create an embedding model using `GCNInfoMax.embedding_model` to obtain the node embeddings. Then we use logistic regression to predict which class the node belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb_in, x_emb_out = fullbatch_infomax_embedding_model(model)\n",
    "emb_model = Model(inputs=x_emb_in, outputs=x_emb_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    node_subjects, train_size=0.1, test_size=None, stratify=node_subjects\n",
    ")\n",
    "\n",
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "\n",
    "train_targets = target_encoding.fit_transform(train_subjects)\n",
    "test_targets = target_encoding.transform(test_subjects)\n",
    "\n",
    "test_gen = generator.flow(test_subjects.index)\n",
    "train_gen = generator.flow(train_subjects.index)\n",
    "\n",
    "test_embeddings = emb_model.predict(test_gen)\n",
    "train_embeddings = emb_model.predict(train_gen)\n",
    "\n",
    "lr = LogisticRegression(multi_class=\"auto\", solver=\"lbfgs\")\n",
    "lr.fit(train_embeddings, train_subjects)\n",
    "\n",
    "y_pred = lr.predict(test_embeddings)\n",
    "print(\"Test classification accuracy:\", (y_pred == test_subjects).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation with TSNE\n",
    "\n",
    "Here we visualize the node embeddings with TSNE. As you can see below, the Deep Graph Infomax model produces well separated embeddings using unsupervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = emb_model.predict(generator.flow(G.nodes()))\n",
    "\n",
    "y = np.argmax(target_encoding.transform(node_subjects), axis=1)\n",
    "trans = TSNE(n_components=2)\n",
    "emb_transformed = pd.DataFrame(trans.fit_transform(all_embeddings), index=list(G.nodes()))\n",
    "emb_transformed[\"label\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(\n",
    "    emb_transformed[0],\n",
    "    emb_transformed[1],\n",
    "    c=emb_transformed[\"label\"].astype(\"category\"),\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "ax.set(aspect=\"equal\", xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "plt.title(\"TSNE visualization of GCN embeddings for cora dataset\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
